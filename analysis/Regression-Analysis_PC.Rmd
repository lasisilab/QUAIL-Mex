---
title: "Linear Regression Analysis"
author: "Junhui He, edited by Paloma C."
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = TRUE)
```

```{r package, include = FALSE}
# load packages
library(dplyr)
library(mice)
library(ggplot2)
library(naniar)
library(glmnet)
library(knitr)
```

## 1 Introduction

Our **research questions** are:

A) What variables measured using Paloma's questionnaires are good predictors of HWISE total scores?

B) What HWISE questions are good predictors of alternative water insecurity measurements, such as hours of water supply (HRS_WEEK), or type of supply (continuous or intermittent, W_WC_WI)?

C) Does water insecurity has any association with Perceived stress scores (PSS)? If so, what variables/aspects of water insecurity are driving this stress levels?


Here I repeat the analyses conducted by Junhui He, but adding and removing a few variables that could make more sense as predictors of the Total HWISE score or Total PSS score. These are the two linear regression models we run earlier:

1. HW_TOTAL ~ D_AGE + D_HH_SIZE + D_CHLD + HLTH_SMK + HLTH_CPAIN_CAT + HLTH_CDIS_CAT + SES_SC_Total

2. PSS_TOTAL ~ D_AGE + D_HH_SIZE + D_CHLD + HLTH_SMK + HLTH_CPAIN_CAT + HLTH_CDIS_CAT + SES_SC_Total

The two new linear regression models are different from the previous ones:

1. Removed HLTH_SMK, HLTH_CPAIN_CAT, and HLTH_CDIS_CAT

2. Added D_LOC_TIME, SEASON, W_WS_LOC, W_WC_WI, HRS_WEEK

3. Added HWISE_TOTAL as potential predictor of PSS

## 1.b Variable descriptions for quick reference

Ordered alphabetically

```{r table info variables, echo=FALSE, results='asis'}
# Load necessary package
library(knitr)

# Create a data frame with variable descriptions, classes, and additional details
variables_info <- data.frame(
  Variable = c("D_AGE", "D_CHLD", "D_HH_SIZE", "D_LOC_TIME", "HLTH_CDIS_CAT", "HLTH_CPAIN_CAT", 
               "HLTH_SMK", "HRS_WEEK", "HW_TOTAL", "MX28_WQ_COMP", "PSS_TOTAL", "SEASON", 
               "SES_SC_Total", "W_WS_LOC", "W_WC_WI"),
  Description = c("Participants' age", 
                  "Number of children participant has birthed",
                  "Household size",
                  "For how long have you lived in this neighborhood?",
                  "Presence of chronic disease",
                  "Presence of chronic pain",
                  "Tobacco smoker",
                  "Hours of water supply in the household per week",
                  "Sum of all 12-items in HWISE questionnaire",
                  "Perception of water service as worse, same, or better than rest of Mexico City",
                  "Total Perceived Stress Score",
                  "Fall or Spring (when data collection happened)",
                  "Socioeconomic status score",
                  "Classification of neighborhoods as water secure or insecure",
                  "Classification of water supply as continuous or intermittent"),
  Class = c("Numeric", "Numeric", "Numeric", "Numeric", "Categorical (Binary)", "Categorical (Binary)", 
            "Categorical (Binary)", "Numeric", "Numeric", "Categorical (Ordinal)", "Numeric", "Categorical (Binary)",
            "Numeric", "Categorical (Binary)", "Categorical (Binary)"),
  Values = c("18:49", 
              "0:8", 
              "2:40", 
              "1:46 (years)", 
              "1 = yes, 0 = no", 
              "1 = yes, 0 = no", 
              "1 = yes, 0 = no", 
              "0:168", 
              "0:27", 
              "0 = worse, 1 = same, 2 = better", 
              "-19:19", 
              "Fall = 1, Spring = 0", 
              "25:263", 
              "1 = water insecure, 0 = water secure", 
              "1 = intermittent, 0 = continuous")
)

# Print the table using kable
kable(variables_info, caption = "Variable Descriptions, Classes, and Additional Details")

```


## 2 Data preparation

1. We remove rows with missing data.

2. HW_TOTAL is calculated by adding up all the HWISE scores; PSS_TOTAL is calculated by adding up PSS 1,2,3, 8, 11, 12, 14, and substracting 4,5,6,7,9,10, and 13.

```{r function to count missing info}
# Function to count missing information per varaible
create_na_table <- function(data) {
  # Create a data frame with variable names and count of missing values
  na_info <- data.frame(
    Variable = names(data),
    Missing_Values = colSums(is.na(data))
  ) %>%
    arrange(desc(Missing_Values))
    # Generate a kable table
  kable(na_info, caption = "Number of Missing Values per Variable")
}
```

```{r read-data}

data_path = "data"
# merged dataset
merged_dataset = read.csv(file.path(data_path, "Cleaned_Dataset_Screening_HWISE_PSS_V3.csv"), stringsAsFactors = FALSE)
colnames(merged_dataset)

# Initial number of unique participants
cat("Initial number of unique participants:", length(unique(merged_dataset$ID)), "\n") 
cat("Initial number of variables:", ncol(merged_dataset), "\n") 

# Keep only numeric data
for (i in 1:dim(merged_dataset)[2]) {
  merged_dataset[[i]] = as.numeric(merged_dataset[[i]])
}

create_na_table(merged_dataset)
# create regression dataset
reg_dataset = merged_dataset[c(1, 2:4, 6:9, 11:13, 40:45)]


# remove missing rows
reg_dataset = na.omit(reg_dataset)


cat("Final number of unique participants:", length(unique(reg_dataset$ID)), "\n") 
cat("Final number of variables:", ncol(reg_dataset), "\n") 

colnames(reg_dataset)
```

## 3 Results

### 3.1 HWISE scores, variable set 1

The regression results for HW is summarized as follows.

```{r regression-HW, include=TRUE}
HW_lm = lm(HW_TOTAL ~ D_AGE+D_HH_SIZE+D_CHLD+SES_SC_Total, data = reg_dataset)
summary(HW_lm)
```
The goodness-of-fit for HW regression is given as follow.

```{r goodness-of-fit-HW, include=TRUE}
ggplot() + geom_point(aes(reg_dataset$HW_TOTAL, HW_lm$fitted.values)) +
  geom_abline(linetype='longdash', linewidth=1) +
  xlab("Ground truth of HW_TOTAL") + 
  ylab("Fitted value") + 
  ggtitle("Goodness of Fit for HW_TOTAL", subtitle = "(Fitted value versus ground truth for HW_TOTAL)")
```
### 3.2 HWISE scores, variable set 2

```{r regression-HW-V2, include=TRUE}
HW_lm = lm(HW_TOTAL ~ D_LOC_TIME+SEASON+W_WS_LOC+W_WC_WI+HRS_WEEK+D_AGE+D_HH_SIZE+D_CHLD+SES_SC_Total, data = reg_dataset)
summary(HW_lm)
```


The goodness-of-fit for HW regression is given as follow.

```{r goodness-of-fit-HW-V2, include=TRUE}
ggplot() + geom_point(aes(reg_dataset$HW_TOTAL, HW_lm$fitted.values)) +
  geom_abline(linetype='longdash', linewidth=1) +
  xlab("Ground truth of HW_TOTAL") + 
  ylab("Fitted value") + 
  ggtitle("Goodness of Fit for HW_TOTAL", subtitle = "(Fitted value versus ground truth for HW_TOTAL)")
```

### 3.2 HWISE scores, variable set 3

```{r regression-HW-V3, include=TRUE}
HW_lm = lm(HW_TOTAL ~ SEASON+W_WS_LOC+W_WC_WI+HRS_WEEK+D_AGE+D_HH_SIZE+D_CHLD+SES_SC_Total, data = reg_dataset)
summary(HW_lm)
```

The goodness-of-fit for HW regression is given as follow.

```{r goodness-of-fit-HW-3, include=TRUE}
ggplot() + geom_point(aes(reg_dataset$HW_TOTAL, HW_lm$fitted.values)) +
  geom_abline(linetype='longdash', linewidth=1) +
  xlab("Ground truth of HW_TOTAL") + 
  ylab("Fitted value") + 
  ggtitle("Goodness of Fit for HW_TOTAL", subtitle = "(Fitted value versus ground truth for HW_TOTAL)")

```

### 3.2 HWISE scores, variable set 4

```{r regression-HW-V4, include=TRUE}
HW_lm = lm(HW_TOTAL ~ MX8_TRUST+MX28_WQ_COMP+ SEASON+W_WS_LOC+W_WC_WI+HRS_WEEK+D_CHLD+SES_SC_Total+PSS_TOTAL, data = reg_dataset)
summary(HW_lm)
```

The goodness-of-fit for HW regression is given as follow.

```{r goodness-of-fit-HW-4, include=TRUE}
ggplot() + geom_point(aes(reg_dataset$HW_TOTAL, HW_lm$fitted.values)) +
  geom_abline(linetype='longdash', linewidth=1) +
  xlab("Ground truth of HW_TOTAL") + 
  ylab("Fitted value") + 
  ggtitle("Goodness of Fit for HW_TOTAL", subtitle = "(Fitted value versus ground truth for HW_TOTAL)")

```


### 3.3 PSS

The regression results for PSS is summarized as follows.

```{r regression-PSS, include=TRUE}
PSS_lm = lm(PSS_TOTAL ~ MX28_WQ_COMP+MX8_TRUST+MX26_EM_HHW_TYPE+SEASON+W_WS_LOC+W_WC_WI+HRS_WEEK+D_AGE+D_HH_SIZE+D_CHLD+SES_SC_Total+HW_TOTAL, data = reg_dataset)
summary(PSS_lm)
```

The goodness-of-fit for PSS regression is given as follow.


```{r goodness-of-fit-PSS, include=TRUE}
ggplot() + geom_point(aes(reg_dataset$PSS_TOTAL, PSS_lm$fitted.values)) + geom_abline(linetype='longdash', linewidth=1) + xlab("Ground truth of PSS_TOTAL") + ylab("Fitted value") + ggtitle("Goodness of Fit for PSS_TOTAL", subtitle = "(Fitted value versus ground truth for HW_TOTAL)")
```

### 3.4 Predictors for hours of water supply

WORK IN PROGRESS
I intend to add each HWISE question in these models

```{r regression-HW-V5, include=TRUE}
HW_lm = lm(HRS_WEEK ~ MX28_WQ_COMP+ D_LOC_TIME+SEASON+W_WS_LOC+W_WC_WI+HW_TOTAL+D_AGE+D_HH_SIZE+D_CHLD+SES_SC_Total, data = reg_dataset)
summary(HW_lm)
```

The goodness-of-fit for HW regression is given as follow.

```{r goodness-of-fit-HW-5, include=TRUE}
ggplot() + geom_point(aes(reg_dataset$HRS_WEEK, HW_lm$fitted.values)) +
  geom_abline(linetype='longdash', linewidth=1) +
  xlab("Ground truth of HRS_WEEK") + 
  ylab("Fitted value") + 
  ggtitle("Goodness of Fit for HRS_WEEK", subtitle = "(Fitted value versus ground truth for HRS_WEEK)")

```


### 3.5 Predictors for perception of W. supply as better, same or worse

WORK IN PROGRESS --> outcome variable is categorical, can't be runned as other vars


### 4 Feature selection

Using Elastic-Net Algorithm with $\alpha=0.5$, the selected predictors for HW_TOTAL include D_LOC_TIME, D_CHILD, SES_SC_TOTAL, SEASON, W_WS_LOC, W_WC_WI, and HRS_WEEK.


```{r HW-elastic-net, include=TRUE}
x_HW = as.matrix(reg_dataset[c(2:8, 10:11)])
HW_TOTAL = c(reg_dataset$HW_TOTAL)
set.seed(123)
# Try more alphas to find the best value
alpha_1 = 0.5
cv_HW_net = cv.glmnet(x_HW, HW_TOTAL, alpha = alpha_1) # 10-fold cross validation
# cv_HW_net$lambda.min
HW_final_model = glmnet(x_HW, HW_TOTAL, alpha = alpha_1, lambda = cv_HW_net$lambda.min)
 coef(HW_final_model)
```


```{r PSS-elastic-net}
x_PSS = as.matrix(reg_dataset[c(2:11)])
PSS_TOTAL = c(reg_dataset$PSS_TOTAL)
set.seed(123)
# Try more alphas to find the best value
alpha_2 = 0.5
cv_PSS_net = cv.glmnet(x_PSS, PSS_TOTAL, alpha = alpha_2) # 10-fold cross validation
PSS_final_model = glmnet(x_PSS, PSS_TOTAL, alpha = alpha_2, lambda = cv_PSS_net$lambda.min)
 coef(PSS_final_model)
```

## 5 Discussion

### 5.1 Comments on results

1. Unfortunately, the coefficient estimates are not significant except for a few predictors. This indicates the linear dependency between the response (HW_TOTAL or PSS_TOTAL) and the predictors are not significant.

2. Based on the goodness-of-fit figures, the predictive performance is really bad, which is consistent with the last comment.

### 5.2 Questions

1. Is it reasonable to use HW_TOTAL or PSS_TOTAL as response variables and other aforementioned variables as predictors? If not, how should I choose response variables and predictors?

2. Previously, I mentioned feature selection, a method used to identify the most influential variables among a set of predictors. Here, "the most influential variable" refers to one that has a significant impact on the response. However, since your cleaned dataset contains only eight predictors, I believe feature selection is unnecessary. Moreover, feature selection is typically employed to prevent overfitting, whereas our primary problem is underfitting.
